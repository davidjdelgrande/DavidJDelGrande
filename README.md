## Hi there ðŸ‘‹
# David J. Del Grande

**AI Evaluation Specialist | Search Quality, Safety & Prompt Systems | Editorial QA for LLMs**

I work at the intersection of **editorial judgment** and **AI evaluation**, focusing on how large language models behave in real-world systems. My background in journalism informs how I approach model assessment: precise language, explicit reasoning, and structured rationales that engineers and quality teams can use to debug, calibrate, and improve AI outputs.

My work spans **AI Search quality**, **LLM safety and bias evaluation**, and **prompt-driven reasoning systems**, with an emphasis on clarity, consistency, and auditability.

---

## Focus Areas

- **AI Search Quality & Relevance**
  - Entity and result descriptions
  - Ranking and relevance judgment
  - Accuracy, completeness, and clarity in search-style outputs

- **LLM Evaluation & Safety**
  - Hallucination detection
  - Instruction-following failures
  - Safety guideline over- and under-triggering
  - Political and contextual asymmetry analysis

- **Prompt & Rationale Systems**
  - Prompt patterns for clarity and disambiguation
  - Evaluation rubrics and decision frameworks
  - Rationale-first annotation designed for engineering feedback loops

- **Editorial QA for AI**
  - AP-style precision
  - Fact sensitivity and contextual accuracy
  - Consistent tone and explanatory quality under ambiguity

---

## What Iâ€™m Building (Next 12 Months)

- A compact **AI Evaluation Toolkit**:
  - Search relevance rubrics
  - Safety and bias evaluation frameworks
  - Rationale templates aligned with engineering workflows

- A set of **sanitized case studies**:
  - Common LLM failure modes
  - Search description inaccuracies
  - Bias and asymmetry patterns
  - Clear explanations of why decisions were made

- A **prompt-pattern library**:
  - Search description refinement
  - Hallucination reduction
  - Safer and more consistent completions

- Lightweight **data-review notebooks**:
  - Basic aggregation and filtering of evaluation outcomes
  - Error-category summaries for calibration and reporting

---

## Working Principles

- **Reasoning is the product.**
- **Clarity beats cleverness.**
- **Consistency is a feature, not a constraint.**
- **Evaluation should be explainable, repeatable, and useful to engineers.**

---

## Contact

- LinkedIn: https://www.linkedin.com/in/davidjdelgrande/
<!--
**davidjdelgrande/DavidJDelGrande** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->
